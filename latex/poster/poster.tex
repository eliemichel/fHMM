%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% baposter Portrait Poster
% LaTeX Template
% Version 1.0 (15/5/13)
%
% Created by:
% Brian Amberg (baposter@brian-amberg.de)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%-------------------------------------------------------------------------------
%---------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%-------------------------------------------------------------------------------
%---------

\documentclass[a0paper,landscape]{baposter}

% Required for specifying captions to tables and figures
\usepackage[font=small,labelfont=bf]{caption} 
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{relsize} % Used for making text smaller in some places
\usepackage{textcomp}
\usepackage{amsmath}  %Nécessaire pour les maths 
\usepackage{amssymb}  %Nécessaire pour les maths 

\graphicspath{{../resources/pictures}} % Directory in which figures are stored

\definecolor{bordercol}{RGB}{40,40,40} % Border color of content boxes
% Background color for the header in the content boxes (left side)
\definecolor{headercol1}{RGB}{186,215,230}
% Background color for the header in the content boxes (right side)
\definecolor{headercol2}{RGB}{80,80,80}
% Text color for the header text in the content boxes
\definecolor{headerfontcol}{RGB}{0,0,0}
% Background color for the content in the content boxes
\definecolor{boxcolor}{RGB}{255, 255, 255}

\newcommand{\hmm}{\textsc{HMM}}
\newcommand{\mcmc}{\textsc{MCMC}}
\newcommand{\fhmm}{\textsc{Factorial HMM}}
\newcommand{\Estep}{\textsc{E-step}}
\newcommand{\Mstep}{\textsc{M-step}}
\newcommand{\EM}{\textsc{EM}}
\newcommand{\meanfield}{\textsc{mean-field}}
\newcommand{\structmeanfield}{\textsc{structured mean-field}}

\begin{document}

\background{ % Set the background to an image (background.pdf)
\begin{tikzpicture}[remember picture,overlay]
% \draw (current page.north west)+(-2em,2em) node[anchor=north west]
% {\includegraphics[height=1.1\textheight]{background}};
\end{tikzpicture}
}

\begin{poster}{
grid=false,
borderColor=bordercol, % Border color of content boxes
% Background color for the header in the content boxes (left side)
headerColorOne=headercol1,
% Background color for the header in the content boxes (right side)
headerColorTwo=headercol2,
% Text color for the header text in the content boxes
headerFontColor=headerfontcol,
% Background color for the content in the content boxes
boxColorOne=boxcolor,
% Specify the rounded corner in the content box headers
headershape=roundedright,
% Font modifiers for the text in the content box headers
headerfont=\Large\sf\bf,
textborder=rectangle,
background=user,
headerborder=open, % Change to closed for a line under the content box headers
boxshade=plain
}
{}
%
%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR NAME
%----------------------------------------------------------------------------------------
%
{\sf\bf Factorial HMM} % Poster title
{Th\'{e}is Bazin, Valentin De Bortoli, \'{E}lie Michel\\ % Author names
% Author email addresses
{\smaller \textlangle{} theis.bazin, vdeborto, elie.michel \textrangle{} @ ens-paris-saclay.fr}}
{\includegraphics[scale=0.2]{../resources/pictures/logos/logo-ens_paris_saclay.png}} % University/lab logo

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\headerbox{Introduction}{name=introduction,column=0,row=0}{

In this poster we present the factorial hidden Markov model, \fhmm{}, an extension of the well-known \hmm{}. Using \hmm{} we assume that the hidden variable is a single multinomial variable. However, this assumption can lead to computational problems. The authors of \cite{ghahramani1997factorial} describe the case where we want to describe $M=30$ binary states. Using \hmm{} the size of the state space is $2^{30}$. Using \textit{distributed} state representations we use only $M=30$ binary states. This property is encoded in the following graph representation.\\
FIGURE\\
We are now focusing on how to do inference in this graphical model.
}

%----------------------------------------------------------------------------------------
%	AN EM ALGORTIHM...
%----------------------------------------------------------------------------------------

\headerbox{An EM algorithm...}{name=methods,column=0,below=introduction}{
We present the following \EM{} algorithm \cite{wu1983convergence}
\begin{description}
\item[\Estep{}] Computing the posterior $p(S_{(t)}^{(m)} \vert Y_{(t)}, \theta)$
\item[\Mstep{}] Updating the parameters $\theta$
\end{description}
\Mstep{} is easy but \Estep{} can be really hard to compute. Recall that in the \hmm{} we used an alpha-beta recursion to deal with the \Estep{}. We present three different ways to deal with \Estep{}:
\begin{itemize}
\item exact inference
\item Gibbs sampling
\item variational methods
\end{itemize}
}

%----------------------------------------------------------------------------------------
%	Exact inference
%----------------------------------------------------------------------------------------

\headerbox{Exact inference}{name=conclusion,column=0,below=methods}{

The exact inference allows us to compute the posterior distribution setting:
\begin{equation}
\left\lbrace
\begin{aligned}
&\alpha_t = p(S_t^1, \dots, S_t^M, Y_{(1,t)} \vert \theta)\\
&\alpha_t^m = p(S_{t-1}^1,\dots,S_{t-1}^m,S_t^{m+1},\dots,S_t^M \vert \theta)\\
&\beta_t = p(Y_{t+1,T} \vert S_t^1, \dots, S_t^M, \theta) \\
&\beta_{t-1}^m = p(Y_{t+1,T} \vert S_t^1,\dots,S_t^m,S_{t-1}^{m+1},\dots,S_{t-1}^M, \theta)
\end{aligned}
\right.
\end{equation}
Thus we have:
\begin{equation}
\gamma_t = P(S_t \vert Y_{(t)}, \theta) \propto \alpha_t \beta_t
\end{equation}
$\alpha_t$ and $\beta_t$ can be computed recursively:\\
GRAPH
}

\headerbox{Gibbs sampling}{name=results1,span=1,column=1,row=0}{
% To reduce this block to 1 column width, remove 'span=2'

To go through we do not need to compute the posterior but only $\langle S_t^m \rangle$, $\langle S_t^m S_t^n \rangle$ and $\langle S_{t-1}^m S_t \rangle$. Using the law of large numbers and \mcmc{} methods we can avoid doing exact inference. We use Gibbs sampling here. Since we have a graph structure the iteration gives:
\begin{equation}
\begin{aligned}
&P(S_{t}^m \vert S_{(-t)}^{(m)}, Y_{(t)},\theta) \propto \\ &P(S_t^m \vert S_{t-1}^m)P(S_{t+1}^m \vert S_t^m)P(Y_t \vert S_t^{(m)})
\end{aligned}
\end{equation}
We use an "impatient" Gibbs sampler, i.e no burn-in samples and only a few.\\
We do not try to reach convergence since this algorithm can be very slow.
}

%----------------------------------------------------------------------------------------
%	VARIATIONAL METHODS
%----------------------------------------------------------------------------------------

\headerbox{Variational methods}{name=var,column=2,row=0}{
Using \mcmc{} we have no information regarding the speed of convergence to the posterior. 
We know introduce a new family of methods which put constraints on the \Estep thus not 
giving the posterior but a \textit{constrained} posterior:
\begin{itemize}
\item \meanfield{}: all hidden states are supposed independent
\item \meanfield{}: we relax the constraint and suppose we have $M$ independent Markov chains.
\end{itemize}
The distance between the true posterior and this approximate proability is given by
$\text{KL}(q \| p(S_{(t)}^{(m)} \vert Y_{(t)}, \theta)$ where KL is the Kullback-Leiber divergence. We iteratively minimize this quantity.
}

%----------------------------------------------------------------------------------------
%	RESULTS
%----------------------------------------------------------------------------------------

\headerbox{Results}{name=results,column=1,span=2,below=var}{
We implemented all the four described methods and tested our implementation computing the 
evolution of the log-likelihood (experiments: $K=2$, $M=3)$.\\
FIGURE\\
Note that the log-likelihood is not always increasing for the variational models since we 
compute an approximated log-likelihood. In \cite{ghahramani1997factorial} the author suggested
using only 10 samples and no burn-in for the Gibbs sampling however we found that adding a little
burn-in smoothed our log-likelihood.\\
FIGURE\\
We also provide the following tabular:
\begin{center}
\begin{tabular}{|c|c|}
\hline
\Estep{} method & Iteration duration (s) \\
\hline \hline
\hmm - exact inference & 0.453  \\ \hline
\fhmm - exact inference & 0.429 \\ \hline
Gibbs sampling &  1.100 \\ \hline
\meanfield & 0.053 \\ \hline
\structmeanfield & 0.319 \\ \hline
\end{tabular}
\end{center}
\newline
As $M$ increases exact inference became intractable and Gibbs sampling inefficient.
Variational methods are still satisfactory though.
}

%----------------------------------------------------------------------------------------
%	CONCLUSION & FUTURE WORK
%----------------------------------------------------------------------------------------

\headerbox{Conclusion and future work...}{name=conclusion,column=1,span=1,below=results}{
In our work we implemented the four methods to compute \Estep{}, investigate the mathematical 
model behind \fhmm{} and experimentally verified the superiority of variationnal models to 
deal with large space state. In a future work, we wish to test our algorithm on real data and
the properties of this model versus \hmm{} on test data.
}

%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%	REFERENCES
%----------------------------------------------------------------------------------------

\headerbox{References}{name=conclusion,column=2,span=1,below=results}{
\bibliographystyle{plain}
\bibliography{../resources/bibliography/bibfile}\
}

%----------------------------------------------------------------------------------------
\end{poster}

\end{document}\grid
